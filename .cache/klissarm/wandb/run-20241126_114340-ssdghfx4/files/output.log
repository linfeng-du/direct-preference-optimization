Creating trainer on process 0 with world size 1
Error executing job with overrides: ['model=pythia28', 'datasets=[hh]', 'loss=sft', 'exp_name=anthropic_dpo_pythia28', 'gradient_accumulation_steps=2', 'batch_size=64', 'eval_batch_size=32', 'sample_during_eval=false', 'model.fsdp_policy_mp=bfloat16']
Traceback (most recent call last):
  File "/home/mila/k/klissarm/direct-preference-optimization/train.py", line 247, in main
    worker_main(0, 1, config, policy, reference_model)
  File "/home/mila/k/klissarm/direct-preference-optimization/train.py", line 96, in worker_main
    trainer = TrainerClass(policy, config, config.seed, config.local_run_dir, reference_model=reference_model, rank=rank, world_size=world_size,hnet_controller = controller)
TypeError: BasicTrainer.__init__() missing 1 required positional argument: 'run_dir'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
