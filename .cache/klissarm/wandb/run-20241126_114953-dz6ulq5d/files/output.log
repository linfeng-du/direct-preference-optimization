Creating trainer on process 0 with world size 1
> /home/mila/k/klissarm/direct-preference-optimization/trainers.py(848)__init__()
-> super().__init__(policy, config, seed, run_dir, reference_model, rank, world_size)
{'seed': 0, 'exp_name': 'anthropic_dpo_pythia28', 'batch_size': 64, 'eval_batch_size': 32, 'debug': False, 'fsdp_port': 56833, 'datasets': ['hh'], 'use_lora': False, 'use_hnet': False, 'train_reward': False, 'wandb': {'enabled': True, 'entity': None, 'project': 'direct-preference-optimization'}, 'local_dirs': ['/scr-ssd', '/scr', '.cache'], 'sample_during_eval': False, 'n_eval_model_samples': 0, 'do_first_eval': True, 'local_run_dir': '.cache/klissarm/anthropic_dpo_pythia28_2024-11-26_11-49-46_059094', 'lr': 0.05, 'gradient_accumulation_steps': 2, 'max_grad_norm': 10.0, 'max_length': 512, 'max_prompt_length': 256, 'n_epochs': 1, 'n_examples': None, 'n_eval_examples': 256, 'trainer': 'FSDPTrainer', 'optimizer': 'AdamW', 'warmup_steps': 150, 'activation_checkpointing': False, 'eval_every': 4928, 'minimum_log_interval_secs': 0.0, 'save': False, 'hnet_type': 'hypernet', 'model': {'name_or_path': 'EleutherAI/pythia-2.8b', 'tokenizer_name_or_path': None, 'archive': None, 'block_name': 'GPTNeoXLayer', 'policy_dtype': 'float32', 'fsdp_policy_mp': 'bfloat16', 'reference_dtype': 'float16'}, 'loss': {'name': 'sft'}, 'hnet': {'d_a': 16, 'alpha': 16, 'd_A': 256, 'n_transformer_heads': 4, 'n_transformer_layers': 8, 'd_emb': 188, 'd_hnet': 128, 'dropout': 0.0, 'use_dummies': True, 'hnet_forward': True, 'd_model': None, 'n_layers': None}, 'lora': {'r': 16}}
Error executing job with overrides: ['model=pythia28', 'datasets=[hh]', 'loss=sft', 'exp_name=anthropic_dpo_pythia28', 'gradient_accumulation_steps=2', 'batch_size=64', 'eval_batch_size=32', 'trainer=FSDPTrainer', 'sample_during_eval=false', 'model.fsdp_policy_mp=bfloat16']
Traceback (most recent call last):
  File "/home/mila/k/klissarm/direct-preference-optimization/train.py", line 242, in main
    worker_main(0, 1, config, policy, reference_model)
  File "/home/mila/k/klissarm/direct-preference-optimization/train.py", line 96, in worker_main
    trainer = TrainerClass(policy, config, config.seed, config.local_run_dir, reference_model=reference_model, rank=rank, world_size=world_size,hnet_controller = controller)
  File "/home/mila/k/klissarm/direct-preference-optimization/trainers.py", line 848, in __init__
    super().__init__(policy, config, seed, run_dir, reference_model, rank, world_size)
  File "/home/mila/k/klissarm/direct-preference-optimization/trainers.py", line 848, in __init__
    super().__init__(policy, config, seed, run_dir, reference_model, rank, world_size)
  File "/cvmfs/ai.mila.quebec/apps/x86_64/debian/python/3.10/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cvmfs/ai.mila.quebec/apps/x86_64/debian/python/3.10/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
