defaults:
  - _self_
  - model: gpt2-instruct
  - loss: dpo
  - adapter: lora

hydra:
  run:
    dir: ${run_dir}

exp_name: debug
run_dir: ./outputs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S_%f}

seed: 0
batch_size: 8
eval_batch_size: 32
save: true

dataset: persona
prepend_persona: true

sample_during_eval: false
n_eval_model_samples: 0
do_first_eval: true

# lr: 5e-2
lr: 5e-5

gradient_accumulation_steps: 1

max_grad_norm: 10.0

max_length: 512
max_prompt_length: 256

n_epochs: 1
n_examples: -1

n_eval_examples: 256

optimizer: AdamW
warmup_steps: 150
activation_checkpointing: false
eval_every: 19840
minimum_log_interval_secs: 0.
