defaults:
  - _self_
  - model: gpt2
  - loss: dpo
  - adaptor: lora

# Experiment name and output directory
hydra:
  run:
    dir: ${local_run_dir}

exp_name: debug
local_run_dir: ./outputs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S_%f}

# random seed for batch sampling
seed: 0

# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
batch_size: 16

# the batch size during evaluation and sampling, if enabled
eval_batch_size: 64

# which dataset(s) to train on; can pass a list like datasets=[hh,shp]
dataset: persona
prepend_persona: true

# whether or not to generate samples during evaluation; disable for FSDP/TensorParallel
#   is recommended, because they are slow
sample_during_eval: false

# how many model samples to generate during evaluation
n_eval_model_samples: 0

# whether to eval at the very beginning of training
do_first_eval: true

# the learning rate
lr: 5e-2

# number of steps to accumulate over for each batch
#   (e.g. if batch_size=4 and gradient_accumulation_steps=2, then we will
#   accumulate gradients over 2 microbatches of size 2)
gradient_accumulation_steps: 1

max_grad_norm: 10.0

max_length: 512
max_prompt_length: 256

n_epochs: 1
n_examples: -1

n_eval_examples: 256

optimizer: AdamW
warmup_steps: 150
activation_checkpointing: false
eval_every: 4960
minimum_log_interval_secs: 0.0
