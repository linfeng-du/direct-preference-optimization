defaults:
  - _self_
  - model: gpt2-instruct
  - loss: dpo
  - adapter: lora

hydra:
  run:
    dir: ${run_dir}

exp_name: debug
run_dir: ./outputs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S_%f}

seed: 0
batch_size: 16
eval_batch_size: 32
save: true

dataset: persona
prepend_persona: ???
n_clusters: null
max_length: 512
max_prompt_length: 256

n_epochs: 1
n_examples: -1

lr: 5e-5

gradient_accumulation_steps: 1
max_grad_norm: 10.

optimizer: AdamW
warmup_steps: 150
activation_checkpointing: false
eval_every: 19840
minimum_log_interval_secs: 0.
